<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"><HTML 
xml:lang="en" xmlns="http://www.w3.org/1999/xhtml"><HEAD><META 
content="IE=11.0000" http-equiv="X-UA-Compatible">

<META name="GENERATOR" content="MSHTML 11.00.9600.18125">
<META http-equiv="Content-Type" content="text/html;charset=utf-8">
<LINK href="style/jemdoc.css" rel="stylesheet" type="text/css">
<TITLE>Zhiqi Huang</TITLE>
</HEAD>

<BODY>
<DIV id="layout-content">
<DIV id="make-narrow">
<DIV id="toptitle">

<H1>Zhiqi Huang </H1></DIV>


<P>
I currently work at AGI startup Moonshot AI (月之暗面).
My prior research experience includes stints at Amazon Alexa AI, Huawei Noah Ark Lab, Tencent AI Lab, and Peking University.
My research interests are MLLM Scaling, Vision Reasoning, Efficient Deep Learning Methods, etc.
<br><br>

<!-- <span style="color: red;">
<strong>We are hiring!</strong>
</span>

<strong>
If you are passionate about developing next generation of AI,
or if you're keen on crafting the next super app,
or if you see yourself fitting into a role at <a href="https://moonshot.jobs.feishu.cn/s/iFdosECp">Moonshot AI</a>,
don't hesitate to get in touch with me!
</strong> -->

<span style="color: red;">
    <strong>Hiring!</strong>
</span>
I am hiring research/engineer interns with strong multimodal research/engineering/data experience to work on vision reasoning and post training!
Please send your CV to "huangzhiqi AT moonshot.cn" or 
apply through our <a href="https://moonshot.jobs.feishu.cn/referral/campus/position/7439647773823387954/detail?token=MzsxNzA5NjM2MzY2Nzg4OzcyNTc3NjM1MzY0NTAzMzg4MTk7MA">official referral link</a>.
</P>

<P>
    <A href="mailto:huangzhiqi@moonshot.cn">[Email]</A>
    <A href="https://scholar.google.com/citations?user=5JGMGCsAAAAJ">[Google Scholar]</A>
</P>


        <h2 id="experience">Experiences</h2>
        <UL>
            <LI>
              <div style="float:left;">Multimodal Researcher @ 
                  <strong>
                      <a href="https://www.moonshot.cn/">Moonshot AI</a>
                  </strong>
              </div>
              <div style="float:right;">Feb. 2024 ~ Now</div> <br/>
              <div style="font-size:0.9em; color:#666;">
                  We built <a href="https://finance.sina.com.cn/roll/2024-12-16/doc-inczrrav2690776.shtml">K1</a>, a multimodal reasoning model
              </div>
            </LI>
        </UL>
        <UL>
          <LI>
            <div style="float:left;">Scaling Researcher @ 
                <strong>
                    <a href="https://www.moonshot.cn/">Moonshot AI</a>
                </strong>
            </div>
            <div style="float:right;">Jul. 2023 ~ Feb. 2024</div> <br/>
            <div style="font-size:0.9em; color:#666;">
                We built <a href="https://kimi.moonshot.cn">Kimi</a>, a long context AI assistant
            </div>
      </LI>
        </UL>
        <UL><LI>
            <div style="float:left;">Senior Researcher @ 
                <strong>
                    <a href="https://www.lightspeed-studios.com/about-us.html">Tencent</a>
                </strong>
            </div>
            <div style="float:right;">Dec. 2021 ~ Jul. 2023</div> <br/>
            <div style="font-size:0.9em; color:#666;">
                Developed AI solutions for game optimization in <a href="https://wildrift.leagueoflegends.com/">LoL</a> and <a href="https://www.pubgmobile.com/">PUBG</a>
            </div>
        </LI></UL>


        <H2>Publications</H2>
        <ul>
            <LI>
                <P>
                    <A href="">Towards Zero-shot Cross-lingual SLU with Syntax-aware Multi-view Contrastive Learning</A>
                    <br>[ICASSP'25] Y. Xie, Z. Xiong, T. Zhang, M. Cui, Y. Li, <strong>Z. Huang</strong>, Z. Zhu
                </P>
            </LI>
            <LI>
                <P>
                    <A href="https://2024.acmmm.org/">FD2Talk: Towards Generalized Talking Head Generation with Facial Decoupled Diffusion Model</A>
                    <br>[ACMMM'24] Z. Yao, X. Cheng, <strong>Z. Huang</strong>
                </P>
            </LI>
            <LI>
                <P>
                    <A href="https://openreview.net/forum?id=djkKU3NIqH&noteId=djkKU3NIqH">Mixture of Bidirectional Adapter for Multi-modal Sarcasm Detection</A>
                    <br>[ACMMM'24] Y. Xie, Z. Zhu, X. Chen, Z. Chen, <strong>Z. Huang</strong>
                </P>
            </LI>
            <LI>
                <P>
                    <A href="https://2024.emnlp.org/">Dual-oriented Disentangled Network with Counterfactual Intervention for Multimodal Intent Detection</A>
                    <br>[EMNLP'24] Z. Chen, Z. Zhu, X. Zhuang, <strong>Z. Huang</strong>, Y. Zou
                </P>
            </LI>
            <LI>
                <P>
                    <A href="https://2024.emnlp.org/">Advancing End-to-End Spoken Language Understanding with the Power of Large Language Models</A>
                    <br>[EMNLP'24] X. Cheng, Z. Zhu, Z. Chen, X. Zhuang, <strong>Z. Huang</strong>, Y. Zou
                </P>
            </LI>
            <LI>
                <P>
                    <A href="https://cikm2024.org/">Divide and Conquer: Scenario-aware Label Graph Interaction for Multi-intent Spoken Language Understanding</A>
                    <br>[CIKM'24] Z. Zhu, X. Cheng, Z. Chen, Z. Wang, <strong>Z. Huang</strong>, Y. Zou
                </P>
            </LI>
            <LI>
                <P>
                    <A href="https://conferences.miccai.org/2024/en/">Multivariate Cooperative Game for Image-Report Pairs: Hierarchical Semantic Alignment for Medical Report Generation</A>
                    <br>[MICCAI'24] Z. Zhu, X. Cheng, Y. Zhang, Z. Chen, Q. Long, H. Li, <strong>Z. Huang</strong>, X. Wu, Y. Zheng
                </P>
            </LI>
            <LI>
                <P>
                    <A href="">Code-Switching Can be Better Aligners: Advancing Cross-Lingual SLU through Representation-Level and Prediction-Level Alignment</A>
                    <br>[ACL'24] Z. Zhu, X. Cheng, Z. Chen, X. Zhuang, <strong>Z. Huang</strong>, Y. Zou
                </P>
            </LI>
            <LI>
                <P>
                    <A href="">Towards ASR-Robust Spoken Language Understanding via Mixture-of-Experts</A>
                    <br>[ACL'24] X. Cheng, Z. Zhu, X. Zhuang, Z. Chen, <strong>Z. Huang</strong>, Y. Zou
                </P>
            </LI>
            <LI>
                <P>
                   <A href="">KC-Prompt: End-to-end Knowledge-Complementary Prompting for Rehearsal-free Continual Learning</A>
                   <br>[ICASSP'24] Y. Li, Y. Liu, X. Cheng, Z. Zhu, H. Li, B. Yang, <strong>Z. Huang</strong>
                </P>
            </LI>
            <LI>
                <P>
                    <A href="">Towards Multimodal Sentiment Analysis via Two-Stage Bottleneck Filtering and Optimal Transport</A>
                    <br>[COLING'24] Y. Xie, Z. Zhu, X. Lu, <strong>Z. Huang</strong>, H. Xiong
                </P>
            </LI>
            <LI>
                <P>
                    <A href="">Alignment before Awareness: Towards Visual Question Localized-Answering in Robotic Surgery via Optimal Transport and Answer Semantics</A>
                    <br>[COLING'24] Z. Zhu, Y. Zhang, X. Cheng, <strong>Z. Huang</strong>, D, Xu, X. Wu, Y. Zheng
                </P>
            </LI>
            <LI>
                <P>
                    <A href="">Zero-Shot Natural Language Understanding via Large Language Models</A>
                    <br>[COLING'24] Z. Zhu, X. Cheng, H. An, Z. Wang, D. Chen, <strong>Z. Huang</strong>
                </P>
            </LI>
            <LI>
                <P>
                    <A href="">Towards Multi-modal Sarcasm Detection via Disentangled Multi-grained Multi-modal Distilling</A>
                    <br>[COLING'24] Z. Zhu, X. Cheng, G. Hu, Y. Li, <strong>Z. Huang</strong>, Y. Zou
                </P>
            </LI>
            <LI>
                <P>
                    <A href="">Knowledge-enhanced Prompt Tuning for Dialogue-based Relation Extraction with Trigger and Label Semantic</A>
                    <br>[COLING'24] H. An, Z. Zhu, X. Cheng, <strong>Z. Huang</strong>, Y. Zou
                </P>
            </LI>
            <LI>
                <P>
                    <A href="https://aclanthology.org/2023.findings-emnlp.794/">Syntax Matters: Towards Spoken Language Understanding via Syntax-Aware Attention</A>
                    <br>[EMNLP'23 Finding] Y. Xie, Z. Zhu, X. Cheng, <strong>Z. Huang</strong>, D. Chen
                </P>
            </LI>
            <LI>
                <P>
                    <A href="https://aclanthology.org/2023.emnlp-main.486/">Enhancing Code-Switching for Cross-lingual SLU: A Unified View of Semantic and Grammatical Coherence</A>
                    <br>[EMNLP'23] Z. Zhu, X. Cheng, <strong>Z. Huang</strong>, D. Chen, Y. Zou
                </P>
            </LI>
            <LI>
                <P>
                    <A href="https://aclanthology.org/2023.findings-acl.793/">Towards Unified SLU Decoding via Label-aware Compact Linguistics Representations</A>
                    <br>[ACL'23 Finding] Z. Zhu, X. Cheng, <strong>Z. Huang</strong>, D. Chen, Y. Zou
                </P>
            </LI>
            <LI>
                <P>
                    <A href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/zhu23_interspeech.pdf">Mix before Align: Towards Zero-shot Cross-lingual Sentiment Analysis via Soft-Mix and Multi-View Learning</A>
                    <br>[INTERSPEECH'23] Z. Zhu, X. Cheng, D. Chen, <strong>Z. Huang</strong>, H. Li, Y. Zou
                </P>
            </LI>
            <LI>
                <P>
                    <A href="https://ieeexplore.ieee.org/document/9747553">Leveraging Bilinear Attention to Improve Spoken Language Understanding</A>
                    <br>[ICASSP'22] D. Chen, <strong>Z. Huang</strong>, Y. Zou
                </P>
            </LI>
            <LI>
                <P>
                    <A href="https://www.ijcai.org/proceedings/2022/0565.pdf"> Towards Joint Intent Detection and Slot Filling via Higher-order Attention </A>
                    <br>[IJCAI'22 Oral] D. Chen, <strong>Z. Huang</strong>, X. Wu, S. Ge, Y. Zou
                </P>
            </LI>
            <LI>
                <P>
                    <A href="https://aclanthology.org/2023.findings-emnlp.533/">A Multi-grained Contrastive Learning Framework for ASR-robust Language Understanding</A>
                    <br>[EMNLP'23] <strong>Z. Huang</strong>, D. Chen, Z. Zhu, X. Cheng
                </P>
            </LI>
            <LI>
                <P>
                    <A href="https://www.aclweb.org/anthology/2020.coling-main.310/"> Federated Learning for Spoken Language Understanding </A>
                    <br>[COLING'20 Oral] <strong>Z. Huang</strong>, F. Liu, Y. Zou<br>
                </P>
            </LI>
            <LI>
                <P>
                    <A href="https://arxiv.org/abs/2009.13431"> PIN: A Novel Parallel Interactive Network for Spoken Language Understanding </A>
                    <br>[ICPR'20] P. Zhou, <strong>Z. Huang</strong>, F. Liu, Y. Zou<br>
                </P>
            </LI>
            <LI>
                <P>
                    <A href="https://aclanthology.org/2022.nlp4convai-1.11.pdf"> MTL-SLT: Multi-task Learning for Spoken Language Tasks </A>
                    <br>[ACL'22 @ ConvAI] <strong>Z. Huang</strong>, M. Rao, A. Raju, Z. Zhang, B. Bui, C. Lee
                </P>
            </LI>
            <LI>
                <P>
                    <A href="https://ojs.aaai.org/index.php/AAAI/article/view/17548"> Audio-Oriented Multimodal Machine Comprehension via Dynamic Inter- & Intra-modality Attention </A>
                    <br>[AAAI'21] <strong>Z. Huang</strong>, F. Liu, X. Wu, S. Ge, H. Wang, W. Fan, Y. Zou
                </P>
            </LI>
            <LI>
                <A href="https://ieeexplore.ieee.org/document/9413885/"> Sentiment Injected Iteratively Co-Interactive Network for Spoken Language Understanding </A>
                <br>[ICASSP'21] <strong>Z. Huang</strong>, F. Liu, P. Zhou, Y. Zou<br>
            </LI>
            <LI>
                <P>
                    <A href="https://aclanthology.org/2021.acl-long.509"> GhostBERT: Generate More Features with Cheap Operations for BERT </A>
                    <br>[ACL'21 Oral] <B>Z. Huang</B>, L. Hou, L. Shang, X. Chen, X. Jiang, Q. Liu <BR>
                </P>
            </LI>
            <LI>
                <P>
                    <A href="https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/DynaBERT#dynabert"> DynaBERT: Dynamic BERT with Adaptive Width and Depth </A>
                    <br>[NeurIPS'20 <A href="https://proceedings.neurips.cc/paper/2020/file/6f5216f8d89b086c18298e043bfe48ed-Paper.pdf">Spotlight</A>] L. Hou, <strong>Z. Huang</strong>, L. Shang, X. Jiang, X. Chen, Q. Liu<br>
                </P>
            </LI>

        </ul>


<div id="footer">
  <div id="footer-text"></div>
</div>
<a href="https://hits.seeyoufarm.com"><img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Fmazicwong%2Fmazicwong.github.io&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false"/></a>
    <p>
    <center>
    <div id="clustrmaps-widget" style="width:50%">
        <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=wM1lXzq1_CyBaEP6AWrElFm_3cdXcHbYi7RoeioAVvA&cl=ffffff&w=a"></script>
    </div>
</center>
    </p>
</div>

</DIV></DIV></BODY></HTML>
